{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing neccessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 7s 7s/step - loss: 2.4014 - accuracy: 0.0000e+00 - val_loss: 2.3926 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.3793 - accuracy: 0.5556 - val_loss: 2.3916 - val_accuracy: 0.3333\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.3574 - accuracy: 0.4444 - val_loss: 2.3905 - val_accuracy: 0.3333\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.3350 - accuracy: 0.4444 - val_loss: 2.3893 - val_accuracy: 0.3333\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.3117 - accuracy: 0.4444 - val_loss: 2.3879 - val_accuracy: 0.3333\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.2868 - accuracy: 0.4444 - val_loss: 2.3862 - val_accuracy: 0.3333\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.2598 - accuracy: 0.4444 - val_loss: 2.3842 - val_accuracy: 0.3333\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.2301 - accuracy: 0.4444 - val_loss: 2.3819 - val_accuracy: 0.3333\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.1972 - accuracy: 0.4444 - val_loss: 2.3791 - val_accuracy: 0.3333\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.1604 - accuracy: 0.4444 - val_loss: 2.3761 - val_accuracy: 0.3333\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.1189 - accuracy: 0.4444 - val_loss: 2.3727 - val_accuracy: 0.3333\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 2.0720 - accuracy: 0.4444 - val_loss: 2.3694 - val_accuracy: 0.3333\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.0187 - accuracy: 0.4444 - val_loss: 2.3665 - val_accuracy: 0.3333\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.9583 - accuracy: 0.4444 - val_loss: 2.3650 - val_accuracy: 0.3333\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.8898 - accuracy: 0.4444 - val_loss: 2.3666 - val_accuracy: 0.3333\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.8130 - accuracy: 0.4444 - val_loss: 2.3741 - val_accuracy: 0.3333\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.7284 - accuracy: 0.4444 - val_loss: 2.3931 - val_accuracy: 0.3333\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.6383 - accuracy: 0.4444 - val_loss: 2.4336 - val_accuracy: 0.3333\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.5492 - accuracy: 0.4444 - val_loss: 2.5130 - val_accuracy: 0.3333\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.4729 - accuracy: 0.4444 - val_loss: 2.6557 - val_accuracy: 0.3333\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.4268 - accuracy: 0.4444 - val_loss: 2.8705 - val_accuracy: 0.3333\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.4185 - accuracy: 0.4444 - val_loss: 3.0977 - val_accuracy: 0.3333\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.4192 - accuracy: 0.4444 - val_loss: 3.2597 - val_accuracy: 0.3333\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.3938 - accuracy: 0.4444 - val_loss: 3.3408 - val_accuracy: 0.3333\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 1.3415 - accuracy: 0.4444 - val_loss: 3.3634 - val_accuracy: 0.3333\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.2796 - accuracy: 0.4444 - val_loss: 3.3578 - val_accuracy: 0.3333\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.2232 - accuracy: 0.4444 - val_loss: 3.3498 - val_accuracy: 0.3333\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.1787 - accuracy: 0.4444 - val_loss: 3.3563 - val_accuracy: 0.3333\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.1455 - accuracy: 0.4444 - val_loss: 3.3861 - val_accuracy: 0.3333\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.1190 - accuracy: 0.4444 - val_loss: 3.4435 - val_accuracy: 0.3333\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.0944 - accuracy: 0.4444 - val_loss: 3.5310 - val_accuracy: 0.3333\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.0680 - accuracy: 0.4444 - val_loss: 3.6517 - val_accuracy: 0.3333\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.0374 - accuracy: 0.4444 - val_loss: 3.8095 - val_accuracy: 0.3333\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.0014 - accuracy: 0.5556 - val_loss: 4.0095 - val_accuracy: 0.3333\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.9603 - accuracy: 0.5556 - val_loss: 4.2567 - val_accuracy: 0.3333\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.9151 - accuracy: 0.5556 - val_loss: 4.5553 - val_accuracy: 0.3333\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.8677 - accuracy: 0.6667 - val_loss: 4.9059 - val_accuracy: 0.3333\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.8201 - accuracy: 0.6667 - val_loss: 5.3025 - val_accuracy: 0.3333\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.7741 - accuracy: 0.7778 - val_loss: 5.7297 - val_accuracy: 0.3333\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.7306 - accuracy: 0.7778 - val_loss: 6.1605 - val_accuracy: 0.3333\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6890 - accuracy: 0.7778 - val_loss: 6.5611 - val_accuracy: 0.3333\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.6473 - accuracy: 0.7778 - val_loss: 6.9016 - val_accuracy: 0.3333\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6031 - accuracy: 0.7778 - val_loss: 7.1679 - val_accuracy: 0.3333\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.5559 - accuracy: 0.8889 - val_loss: 7.3664 - val_accuracy: 0.3333\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.5078 - accuracy: 0.8889 - val_loss: 7.5193 - val_accuracy: 0.3333\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4627 - accuracy: 0.8889 - val_loss: 7.6560 - val_accuracy: 0.3333\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4218 - accuracy: 1.0000 - val_loss: 7.8034 - val_accuracy: 0.3333\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.3835 - accuracy: 1.0000 - val_loss: 7.9775 - val_accuracy: 0.3333\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3451 - accuracy: 1.0000 - val_loss: 8.1779 - val_accuracy: 0.3333\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3071 - accuracy: 1.0000 - val_loss: 8.3897 - val_accuracy: 0.3333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initial model testing data \n",
    "# data = [\n",
    "#     {'english': 'I am fine, and you?', 'akan': 'Me ho ye'},\n",
    "#     {'english': 'Good morning', 'akan': 'Mema wo akye'},\n",
    "#     {'english': 'Thank you', 'akan': 'Medase'},\n",
    "#     {'english': 'What is your name?', 'akan': 'Wo din de sen?'},\n",
    "# ]\n",
    "# current data - 25,000 parallel sentences\n",
    "data = pd.read_json('dataset.json')\n",
    "\n",
    "# Extracting sentences into respective variables\n",
    "english_texts = [item['english'] for item in data]\n",
    "akan_texts = [item['akan'] for item in data]\n",
    "\n",
    "# English Sentences Tokenization\n",
    "eng_tokenizer = Tokenizer(filters='')\n",
    "eng_tokenizer.fit_on_texts(english_texts)\n",
    "eng_sequences = eng_tokenizer.texts_to_sequences(english_texts)\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_padded = pad_sequences(eng_sequences, padding='post')\n",
    "\n",
    "# Akan Sentences Tokenization\n",
    "akan_tokenizer = Tokenizer(filters='')\n",
    "akan_tokenizer.fit_on_texts(akan_texts)\n",
    "akan_sequences = akan_tokenizer.texts_to_sequences(akan_texts)\n",
    "akan_vocab_size = len(akan_tokenizer.word_index) + 1\n",
    "akan_padded = pad_sequences(akan_sequences, padding='post')\n",
    "\n",
    "# Training, Testing & Validation Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(eng_padded, akan_padded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Parameters\n",
    "embedding_dim = 64\n",
    "units = 128\n",
    "\n",
    "# Model Layers (Encoder-Decoder Layers) - return_sequences and return_state\n",
    "encoder_inputs = tf.keras.Input(shape=(None,))\n",
    "encoder_embedding = Embedding(eng_vocab_size, embedding_dim)(encoder_inputs)\n",
    "encoder_gru = GRU(units, return_sequences=False, return_state=True)\n",
    "encoder_outputs, encoder_state = encoder_gru(encoder_embedding)  # Ensure encoder_state is returned\n",
    "\n",
    "decoder_inputs = tf.keras.Input(shape=(None,))\n",
    "decoder_embedding = Embedding(akan_vocab_size, embedding_dim)(decoder_inputs)\n",
    "decoder_gru = GRU(units, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _ = decoder_gru(decoder_embedding, initial_state=encoder_state)  # Pass encoder_state\n",
    "decoder_dense = Dense(akan_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Compiling Model\n",
    "model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model Training\n",
    "model.fit([X_train, y_train[:, :-1]], y_train[:, 1:], epochs=50, batch_size=32, validation_data=([X_test, y_test[:, :-1]], y_test[:, 1:]))\n",
    "\n",
    "# Saving Model\n",
    "model.save('english_to_akan_gru.h5')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, None, 64)     896         ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)        (None, None, 64)     704         ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " gru_4 (GRU)                    [(None, 128),        74496       ['embedding_4[0][0]']            \n",
      "                                 (None, 128)]                                                     \n",
      "                                                                                                  \n",
      " gru_5 (GRU)                    [(None, None, 128),  74496       ['embedding_5[0][0]',            \n",
      "                                 (None, 128)]                     'gru_4[0][1]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, None, 11)     1419        ['gru_5[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 152,011\n",
      "Trainable params: 152,011\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model Summary - To be sure the layers are arranged in the exact order we want it\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_encoder_decoder(model, units):\n",
    "    \"\"\"Extract encoder and decoder models for inference.\"\"\"\n",
    "    \n",
    "    # Encoder Layer\n",
    "    encoder_inputs = model.input[0]  # English input\n",
    "    encoder_embedding = model.layers[1](encoder_inputs)\n",
    "    encoder_outputs, encoder_state = model.layers[2](encoder_embedding)\n",
    "    encoder_model = tf.keras.Model(encoder_inputs, encoder_state)\n",
    "\n",
    "    # Decoder Layer\n",
    "    decoder_inputs = tf.keras.Input(shape=(None,))\n",
    "    decoder_state_input = tf.keras.Input(shape=(units,))\n",
    "\n",
    "    decoder_embedding = model.layers[3](decoder_inputs)\n",
    "    decoder_gru = model.layers[4]\n",
    "    \n",
    "    decoder_outputs, decoder_state = decoder_gru(decoder_embedding, initial_state=decoder_state_input)\n",
    "    decoder_dense = model.layers[5]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    decoder_model = tf.keras.Model([decoder_inputs, decoder_state_input], [decoder_outputs, decoder_state])\n",
    "\n",
    "    return encoder_model, decoder_model\n",
    "\n",
    "def preprocess_sentence(sentence, tokenizer, max_len):\n",
    "    sequence = tokenizer.texts_to_sequences([sentence])\n",
    "    return pad_sequences(sequence, maxlen=max_len, padding='post')\n",
    "\n",
    "def translate_sentence(encoder_model, decoder_model, sentence, eng_tokenizer, akan_tokenizer, max_len):    \n",
    "    #  Input Preprocesing\n",
    "    encoder_input = preprocess_sentence(sentence, eng_tokenizer, max_len)\n",
    "\n",
    "    # Encoding input sentence\n",
    "    encoder_state = encoder_model.predict(encoder_input)\n",
    "\n",
    "    # Start decoding with \"<start>\" token if applicable, else use 1\n",
    "    decoder_input = np.zeros((1, 1))\n",
    "    decoder_input[0, 0] = akan_tokenizer.word_index.get('<start>', 1)\n",
    "\n",
    "    decoded_sentence = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        output_tokens, state = decoder_model.predict([decoder_input, encoder_state])\n",
    "        predicted_id = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        # Stop at <end> token\n",
    "        if predicted_id == 0:  \n",
    "            break\n",
    "        \n",
    "        word = akan_tokenizer.index_word.get(predicted_id, '')\n",
    "        decoded_sentence.append(word)\n",
    "        \n",
    "        # Updating decoder input for next iteration\n",
    "        decoder_input[0, 0] = predicted_id\n",
    "        \n",
    "        # Updating the state\n",
    "        encoder_state = state  \n",
    "    \n",
    "    return ' '.join(decoded_sentence)\n",
    "\n",
    "# Loading the trained model\n",
    "model = tf.keras.models.load_model('english_to_akan_gru.h5')\n",
    "\n",
    "# Extracting encoder and decoder\n",
    "encoder_model, decoder_model = load_encoder_decoder(model, units=128)\n",
    "\n",
    "# Test translation\n",
    "english_sentence = \"What is your name?\"\n",
    "akan_translation = translate_sentence(encoder_model, decoder_model, english_sentence, eng_tokenizer, akan_tokenizer, max_len=10)\n",
    "\n",
    "print(f\"English: {english_sentence}\")\n",
    "print(f\"Akan: {akan_translation}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
